{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hardware Preview","metadata":{}},{"cell_type":"code","source":"!lscpu\n# Expect Intel Xeon Chip @2.0Ghz with 2 cores 4 threads, 2xT4 wont give you this much CPU power, but more GPU power in predicting samples.\n!nvidia-smi \n# nVidia P100 expected, 2xT4 requires setup encode_multi_process, read it here https://www.sbert.net/docs/package_reference/SentenceTransformer.html\n!python -V \n# Python 3.7.12\n!cmake --version\n# cmake version 3.22.5","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:20:32.389984Z","iopub.execute_input":"2022-12-12T11:20:32.390526Z","iopub.status.idle":"2022-12-12T11:20:37.119021Z","shell.execute_reply.started":"2022-12-12T11:20:32.390415Z","shell.execute_reply":"2022-12-12T11:20:37.117153Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Architecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 48 bits virtual\nCPU(s):                          4\nOn-line CPU(s) list:             0-3\nThread(s) per core:              2\nCore(s) per socket:              2\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           79\nModel name:                      Intel(R) Xeon(R) CPU @ 2.20GHz\nStepping:                        0\nCPU MHz:                         2199.998\nBogoMIPS:                        4399.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       64 KiB\nL1i cache:                       64 KiB\nL2 cache:                        512 KiB\nL3 cache:                        55 MiB\nNUMA node0 CPU(s):               0-3\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Mitigation; PTE Inversion\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nVulnerability Meltdown:          Mitigation; PTI\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no mic\n                                 rocode; SMT Host state unknown\nVulnerability Retbleed:          Mitigation; IBRS\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled v\n                                 ia prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user\n                                  pointer sanitization\nVulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, RSB filling\n                                 , PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n                                 d sse4_1 sse4_2 x2apic movbe popcnt aes xsave a\n                                 vx f16c rdrand hypervisor lahf_lm abm 3dnowpref\n                                 etch invpcid_single pti ssbd ibrs ibpb stibp fs\n                                 gsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms \n                                 invpcid rtm rdseed adx smap xsaveopt arat md_cl\n                                 ear arch_capabilities\n/bin/bash: nvidia-smi: command not found\nPython 3.7.12\ncmake version 3.16.3\n\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q pyvi\n!pip install -q visen\n!pip install -q emoji\n!pip install -q pretty-confusion-matrix\n!pip install -q Cython\n!pip install -q sentence-transformers\n\n\n# !pip uninstall -q -y lightgbm\n# !apt-get -q install -y libboost-all-dev\n# !git clone --quiet --recursive https://github.com/Microsoft/LightGBM\n!git clone --quiet https://github.com/coccoc/coccoc-tokenizer.git","metadata":{"id":"Iw28u2cN5rjx","outputId":"e9408877-2e7f-484b-f81f-3313c3c07801","execution":{"iopub.status.busy":"2022-12-12T11:20:37.123339Z","iopub.execute_input":"2022-12-12T11:20:37.123986Z","iopub.status.idle":"2022-12-12T11:22:20.434134Z","shell.execute_reply.started":"2022-12-12T11:20:37.123893Z","shell.execute_reply":"2022-12-12T11:22:20.432331Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytoolconfig 1.2.2 requires tomli>=2.0; python_version < \"3.11\", but you have tomli 1.2.3 which is incompatible.\nautopep8 1.6.0 requires pycodestyle>=2.8.0, but you have pycodestyle 2.7.0 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!cd coccoc-tokenizer/ && mkdir build\n!cd coccoc-tokenizer/ && cd build && cmake -DBUILD_PYTHON=1 ..\n!cd coccoc-tokenizer/ && cd build && make install\n!cd coccoc-tokenizer/python && python setup.py install\n\n# !cd LightGBM/ && rm -r build\n# !cd LightGBM/ && mkdir build\n# !cd LightGBM/ && cd build && cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\n# !cd LightGBM/ && cd build && make -j$(nproc)\n# !cd LightGBM/python-package/;python setup.py install --precompile\n# !mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n# !rm -r LightGBM","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:22:20.436401Z","iopub.execute_input":"2022-12-12T11:22:20.436905Z","iopub.status.idle":"2022-12-12T11:23:28.696242Z","shell.execute_reply.started":"2022-12-12T11:22:20.436856Z","shell.execute_reply":"2022-12-12T11:23:28.694455Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"-- The C compiler identification is GNU 9.4.0\n-- The CXX compiler identification is GNU 9.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /kaggle/working/coccoc-tokenizer/build\n\u001b[35m\u001b[1mScanning dependencies of target compile_python\u001b[0m\n[ 12%] \u001b[34m\u001b[1mGenerating python/lib\u001b[0m\nrunning install\nrunning build\nrunning build_ext\ncythoning CocCocTokenizer.pyx to CocCocTokenizer.cpp\n/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /kaggle/working/coccoc-tokenizer/python/CocCocTokenizer.pyx\n  tree = Parsing.p_module(s, pxd, full_module_name)\nbuilding 'CocCocTokenizer' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I.. -I/kaggle/working/coccoc-tokenizer/build/auto -O2 -march=native -Wno-cpp -Wno-unused-function -std=c++11 -fPIC -I/opt/conda/include/python3.7m -c CocCocTokenizer.cpp -o build/temp.linux-x86_64-3.7/CocCocTokenizer.o\n\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ -I.. -I/kaggle/working/coccoc-tokenizer/build/auto -O2 -march=native -Wno-cpp -Wno-unused-function -std=c++11 build/temp.linux-x86_64-3.7/CocCocTokenizer.o -o build/lib.linux-x86_64-3.7/CocCocTokenizer.cpython-37m-x86_64-linux-gnu.so\nrunning install_lib\ncreating /kaggle/working/coccoc-tokenizer/build/python\ncreating /kaggle/working/coccoc-tokenizer/build/python/lib\ncreating /kaggle/working/coccoc-tokenizer/build/python/lib/python3.7\ncreating /kaggle/working/coccoc-tokenizer/build/python/lib/python3.7/site-packages\ncopying build/lib.linux-x86_64-3.7/CocCocTokenizer.cpython-37m-x86_64-linux-gnu.so -> /kaggle/working/coccoc-tokenizer/build/python/lib/python3.7/site-packages\nrunning install_egg_info\nWriting /kaggle/working/coccoc-tokenizer/build/python/lib/python3.7/site-packages/CocCocTokenizer-1.4-py3.7.egg-info\n[ 12%] Built target compile_python\n\u001b[35m\u001b[1mScanning dependencies of target dict_compiler\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object CMakeFiles/dict_compiler.dir/utils/dict_compiler.cpp.o\u001b[0m\n[ 37%] \u001b[32m\u001b[1mLinking CXX executable dict_compiler\u001b[0m\n[ 37%] Built target dict_compiler\n\u001b[35m\u001b[1mScanning dependencies of target compile_dict\u001b[0m\n[ 50%] \u001b[34m\u001b[1mGenerating multiterm_trie.dump, syllable_trie.dump, nontone_pair_freq_map.dump\u001b[0m\n[ 50%] Built target compile_dict\n\u001b[35m\u001b[1mScanning dependencies of target vn_lang_tool\u001b[0m\n[ 62%] \u001b[32mBuilding CXX object CMakeFiles/vn_lang_tool.dir/utils/vn_lang_tool.cpp.o\u001b[0m\n[ 75%] \u001b[32m\u001b[1mLinking CXX executable vn_lang_tool\u001b[0m\n[ 75%] Built target vn_lang_tool\n\u001b[35m\u001b[1mScanning dependencies of target tokenizer\u001b[0m\n[ 87%] \u001b[32mBuilding CXX object CMakeFiles/tokenizer.dir/utils/tokenizer.cpp.o\u001b[0m\n[100%] \u001b[32m\u001b[1mLinking CXX executable tokenizer\u001b[0m\n[100%] Built target tokenizer\n\u001b[36mInstall the project...\u001b[0m\n-- Install configuration: \"RELEASE\"\n-- Installing: /usr/local/bin/tokenizer\n-- Installing: /usr/local/bin/vn_lang_tool\n-- Installing: /usr/local/bin/dict_compiler\n-- Installing: /usr/local/include/tokenizer\n-- Installing: /usr/local/include/tokenizer/auxiliary\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_stdint.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_smartptr.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_dlalloc.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_utils.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_config.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_timer.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_traits.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp_memory.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/sparsepp/spp.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/utf8\n-- Installing: /usr/local/include/tokenizer/auxiliary/utf8/unchecked.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/utf8/core.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/utf8/checked.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/utf8.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/tsl\n-- Installing: /usr/local/include/tokenizer/auxiliary/tsl/robin_map.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/tsl/robin_growth_policy.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/tsl/robin_hash.h\n-- Installing: /usr/local/include/tokenizer/auxiliary/tsl/robin_set.h\n-- Up-to-date: /usr/local/include/tokenizer\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/trie\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/da_trie_node.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/multiterm_hash_trie_node.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/hash_trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/multiterm_da_trie_node.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/da_trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/syllable_hash_trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/syllable_hash_trie_node.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/multiterm_da_trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/string_set_trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/syllable_da_trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/multiterm_hash_trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/hash_trie_node.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie/syllable_da_trie_node.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/file_serializer.hpp\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/sparsepp\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/utf8\n-- Installing: /usr/local/include/tokenizer/auxiliary/trie.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/buffered_reader.hpp\n-- Installing: /usr/local/include/tokenizer/auxiliary/vn_lang_tool.hpp\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/tsl\n-- Installing: /usr/local/include/tokenizer/tokenizer.hpp\n-- Installing: /usr/local/include/tokenizer/helper.hpp\n-- Installing: /usr/local/include/tokenizer/token.hpp\n-- Up-to-date: /usr/local/include/tokenizer\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/trie\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/sparsepp\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/utf8\n-- Up-to-date: /usr/local/include/tokenizer/auxiliary/tsl\n-- Installing: /usr/local/include/tokenizer/config.h\n-- Installing: /usr/local/share/tokenizer/dicts_text\n-- Installing: /usr/local/share/tokenizer/dicts_text/vn_lang_tool\n-- Installing: /usr/local/share/tokenizer/dicts_text/vn_lang_tool/numeric\n-- Installing: /usr/local/share/tokenizer/dicts_text/vn_lang_tool/i_and_y.txt\n-- Installing: /usr/local/share/tokenizer/dicts_text/vn_lang_tool/d_and_gi.txt\n-- Installing: /usr/local/share/tokenizer/dicts_text/vn_lang_tool/alphabetic\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/chemical_comp\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/acronyms\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/Freq2NontoneUniFile\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/special_token.weak\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/nontone_pair_freq\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/special_token.strong\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/keyword.freq\n-- Installing: /usr/local/share/tokenizer/dicts_text/tokenizer/vndic_multiterm\n-- Installing: /usr/local/share/tokenizer/dicts\n-- Installing: /usr/local/share/tokenizer/dicts/numeric\n-- Installing: /usr/local/share/tokenizer/dicts/i_and_y.txt\n-- Installing: /usr/local/share/tokenizer/dicts/d_and_gi.txt\n-- Installing: /usr/local/share/tokenizer/dicts/alphabetic\n-- Installing: /usr/local/share/tokenizer/dicts/multiterm_trie.dump\n-- Installing: /usr/local/share/tokenizer/dicts/syllable_trie.dump\n-- Installing: /usr/local/share/tokenizer/dicts/nontone_pair_freq_map.dump\n-- Up-to-date: /usr/local/lib\n-- Installing: /usr/local/lib/python3.7\n-- Installing: /usr/local/lib/python3.7/site-packages\n-- Installing: /usr/local/lib/python3.7/site-packages/CocCocTokenizer-1.4-py3.7.egg-info\n-- Installing: /usr/local/lib/python3.7/site-packages/CocCocTokenizer.cpython-37m-x86_64-linux-gnu.so\nrunning install\nrunning build\nrunning build_ext\nskipping 'CocCocTokenizer.cpp' Cython extension (up-to-date)\nrunning install_lib\ncopying build/lib.linux-x86_64-3.7/CocCocTokenizer.cpython-37m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.7/site-packages\nrunning install_egg_info\nWriting /opt/conda/lib/python3.7/site-packages/CocCocTokenizer-1.4-py3.7.egg-info\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom unicodedata import normalize\nfrom string import punctuation\nimport visen\nimport emoji\nfrom CocCocTokenizer import PyTokenizer\ntokenizer = PyTokenizer(load_nontone_data=True).word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss, accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n# import lightgbm as lgb\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gc\nfrom pretty_confusion_matrix import pp_matrix\nimport torch\nfrom torch import optim\nimport scipy\nimport sys","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:23:28.699695Z","iopub.execute_input":"2022-12-12T11:23:28.700249Z","iopub.status.idle":"2022-12-12T11:23:33.567094Z","shell.execute_reply.started":"2022-12-12T11:23:28.700192Z","shell.execute_reply":"2022-12-12T11:23:33.565294Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"opening_ls = ['[', '{', '⁅', '〈', '⎡', '⎢', '⎣', '⎧', '⎨', '⎩', '❬', '❰', '❲', '❴', '⟦', '⟨', '⟪', '⟬', '⦃', '⦇', '⦉',\n              '⦋', '⦍', '⦏', '⦑', '⦓', '⦕', '⦗', '⧼', '⸂', '⸄', '⸉', '⸌', '⸜', '⸢', '⸤', '⸦', '〈', '《', '「', '『',\n              '【', '〔', '〖', '〘', '〚', '﹛', '﹝', '［', '｛', '｢', '｣']\n\nclosing_ls = [']', '}', '⁆', '〉', '⎤', '⎥', '⎦', '⎫', '⎬', '⎭', '❭', '❱', '❳', '❵', '⟧', '⟩', '⟫', '⟭', '⦄', '⦈', '⦊',\n              '⦌', '⦎', '⦐', '⦒', '⦔', '⦖', '⦘', '⧽', '⸃', '⸅', '⸊', '⸍', '⸝', '⸣', '⸥', '⸧', '〉', '》', '」', '』',\n              '】', '〕', '〗', '〙', '〛', '﹜', '﹞', '］', '｝', '｣']\n\nopening_brackets = {key: '(' for key in opening_ls}\nclosing_brackets = {key: ')' for key in closing_ls}\n\n### constant\nPUNC = '!\\\"#$&()*+,-–−./:;=?@[\\]^_`{|}~”“`°²ˈ‐ㄧ‛∼’'  # remove <> for number_sym and unknown_sym\nNUMBER_SYM = '<NUMBER>'\nUNKNOWN_SYM = ' <UNKNUM> '  # mix number with character, ex: 6.23A\nVIETNAMESE_CHARS = 'aàảãáạăằẳẵắặâầẩẫấậbcdđeèẻẽéẹêềểễếệfghiìỉĩíịjklmnrstoòỏõóọôồổỗốộơờởỡớợpquùủũúụưừửữứựvwxyỳỷỹýỵz'\nre_email = '([a-zA-Z0-9._%+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z]{2,4}(\\\\.?[a-zA-Z]{2,4})?)'\nre_url = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\nre_url2 = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\nre_image = '(tập[\\s_]tin|hình|file|image|imagesize).*?(jpg|svg|png|gif|jpeg|ogg|tif|width)'\nre_num_and_decimal = '[0-9]*[,.\\-]*[0-9]*[,.\\-]*[0-9]*[.,\\-]*[0-9]*[,.\\-]*[0-9]+[.,]?'\nre_unknown = '[a-z]+[\\d]+[\\w]*|[\\d]+[a-z]+[\\w]*'\nre_vnese_txt = r'[^a-z0-9A-ZàáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹýÀÁÃẠẢĂẮẰẲẴẶÂẤẦẨẪẬÈÉẸẺẼÊỀẾỂỄỆĐÌÍĨỈỊÒÓÕỌỎÔỐỒỔỖỘƠỚỜỞỠỢÙÚŨỤỦƯỨỪỬỮỰỲỴỶỸÝ\\s|_]'\nre_truncate_unknown = '(<UNKNUM>\\s*)+'\nspecial_punc = {'”': '\"', '': '', \"’\": \"'\", \"`\": \"'\"}\n\n\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\ndef replace_all(replacer: dict, txt: str) -> str:\n    for old, new in replacer.items():\n        txt = txt.replace(old, new)\n    return txt\n\n\ndef replace_num(text: str) -> str:\n    text = re.sub(re_num_and_decimal, UNKNOWN_SYM, text)\n    return text\n\n\ndef replace_unknown(text: str) -> str:\n    text = re.sub(re_unknown, UNKNOWN_SYM, text)\n    return text\n\n\ndef unicode_normalizer(text, forms: list = ['NFKC', 'NKFD', 'NFC', 'NFD']) -> str:\n    for form in forms:\n        text = normalize(form, text)\n    return text\n\n\ndef normalize_bracket(text: str) -> str:\n    text = replace_all(opening_brackets, text)\n    text = replace_all(closing_brackets, text)\n    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \" \", text)\n    return text\n\n\ndef remove_punc(text: str) -> str:\n    r = re.compile(r'[\\s{}]+'.format(re.escape(PUNC)))\n    text = r.split(text)\n    return ' '.join(i for i in text if i)\n\n\ndef truncate_unknown(text: str) -> str:\n    text = re.sub(re_truncate_unknown, UNKNOWN_SYM, text)\n    return text\n\n\ndef mask(text: str) -> str:\n    text = str(text)\n    text = text.lower()\n    text = text.split('\\n')[0]\n    text = unicode_normalizer(text, [\"NFKC\", \"NFKD\", \"NFD\", \"NFC\"])\n    text = replace_all(special_punc, text)\n    text = normalize_bracket(text)\n    text = replace_unknown(text)\n    text = replace_num(text)\n    text = remove_punc(text)\n    text = truncate_unknown(text)\n    text = re.sub(re_vnese_txt, \" \", text)\n    text = text.strip()\n    return visen.clean_tone(text)","metadata":{"id":"phUxD5yp5sk4","execution":{"iopub.status.busy":"2022-12-12T11:23:33.572365Z","iopub.execute_input":"2022-12-12T11:23:33.573190Z","iopub.status.idle":"2022-12-12T11:23:33.601677Z","shell.execute_reply.started":"2022-12-12T11:23:33.573141Z","shell.execute_reply":"2022-12-12T11:23:33.598989Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"sample_txts = [\n    '〖𝔰𝔬𝔪𝔢 𝔣𝔞𝔫𝔠𝔶 𝔭𝔯𝔬𝔡𝔲𝔠𝔱 𝔫𝔞𝔪𝔢〗',\n    '𝖘𝖔𝖒𝖊 𝖋𝖆𝖓𝖈𝖞 𝖕𝖗𝖔𝖉𝖚𝖈𝖙 𝖓𝖆𝖒𝖊',\n    '❰some product code which will overfit this sample❱ 𝓼𝓸𝓶𝓮 𝓯𝓪𝓷𝓬𝔂 𝓹𝓻𝓸𝓭𝓾𝓬𝓽 𝓷𝓪𝓶𝓮',\n    '𝓈𝑜𝓂𝑒 𝒻𝒶𝓃𝒸𝓎 𝓅𝓇𝑜𝒹𝓊𝒸𝓉 𝓃𝒶𝓂𝑒',\n    '{𝕤𝕠𝕞𝕖 𝕗𝕒𝕟𝕔𝕪 𝕡𝕣𝕠𝕕𝕦𝕔𝕥 𝕟𝕒𝕞𝕖',\n    '☯😝ｓｏｍｅ ｆａｎｃｙ ｐｒｏｄｕｃｔ ｎａｍｅ☯😝',\n    '𝐬𝐨𝐦𝐞 𝐟𝐚𝐧𝐜𝐲 𝐩𝐫𝐨𝐝𝐮𝐜𝐭 𝐧𝐚𝐦𝐞',\n    '𝘀𝗼𝗺𝗲 𝗳𝗮𝗻𝗰𝘆 𝗽𝗿𝗼𝗱𝘂𝗰𝘁 𝗻𝗮𝗺𝗲',\n    '𝘴𝘰𝘮𝘦 𝘧𝘢𝘯𝘤𝘺 𝘱𝘳𝘰𝘥𝘶𝘤𝘵 𝘯𝘢𝘮𝘦',\n    '▄▀▄▀▄▀ 𝙨𝙤𝙢𝙚 𝙛𝙖𝙣𝙘𝙮 𝙥𝙧𝙤𝙙𝙪𝙘𝙩 𝙣𝙖𝙢𝙚 ▄▀▄▀▄▀',\n    '🌌  🎀 𝚜𝚘𝚖𝚎 𝚏𝚊𝚗𝚌𝚢 𝚙𝚛𝚘𝚍𝚞𝚌𝚝 𝚗𝚊𝚖𝚎🌌  🎀 ',\n    '(っ◔◡◔)っ ♥ some fancy product name ♥',\n]\n\nfor sample in sample_txts:\n    print(f'sample \\\"{sample}\\\", after normalize: \\\"{mask(sample)}\\\"')","metadata":{"id":"X4uDz_r_5wRq","outputId":"366d9f61-2469-49d4-af3d-8fe152d9ef6d","execution":{"iopub.status.busy":"2022-12-12T11:23:33.604187Z","iopub.execute_input":"2022-12-12T11:23:33.604778Z","iopub.status.idle":"2022-12-12T11:23:33.705267Z","shell.execute_reply.started":"2022-12-12T11:23:33.604722Z","shell.execute_reply":"2022-12-12T11:23:33.703916Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"sample \"〖𝔰𝔬𝔪𝔢 𝔣𝔞𝔫𝔠𝔶 𝔭𝔯𝔬𝔡𝔲𝔠𝔱 𝔫𝔞𝔪𝔢〗\", after normalize: \"\"\nsample \"𝖘𝖔𝖒𝖊 𝖋𝖆𝖓𝖈𝖞 𝖕𝖗𝖔𝖉𝖚𝖈𝖙 𝖓𝖆𝖒𝖊\", after normalize: \"some fancy product name\"\nsample \"❰some product code which will overfit this sample❱ 𝓼𝓸𝓶𝓮 𝓯𝓪𝓷𝓬𝔂 𝓹𝓻𝓸𝓭𝓾𝓬𝓽 𝓷𝓪𝓶𝓮\", after normalize: \"some fancy product name\"\nsample \"𝓈𝑜𝓂𝑒 𝒻𝒶𝓃𝒸𝓎 𝓅𝓇𝑜𝒹𝓊𝒸𝓉 𝓃𝒶𝓂𝑒\", after normalize: \"some fancy product name\"\nsample \"{𝕤𝕠𝕞𝕖 𝕗𝕒𝕟𝕔𝕪 𝕡𝕣𝕠𝕕𝕦𝕔𝕥 𝕟𝕒𝕞𝕖\", after normalize: \"some fancy product name\"\nsample \"☯😝ｓｏｍｅ ｆａｎｃｙ ｐｒｏｄｕｃｔ ｎａｍｅ☯😝\", after normalize: \"some fancy product name\"\nsample \"𝐬𝐨𝐦𝐞 𝐟𝐚𝐧𝐜𝐲 𝐩𝐫𝐨𝐝𝐮𝐜𝐭 𝐧𝐚𝐦𝐞\", after normalize: \"some fancy product name\"\nsample \"𝘀𝗼𝗺𝗲 𝗳𝗮𝗻𝗰𝘆 𝗽𝗿𝗼𝗱𝘂𝗰𝘁 𝗻𝗮𝗺𝗲\", after normalize: \"some fancy product name\"\nsample \"𝘴𝘰𝘮𝘦 𝘧𝘢𝘯𝘤𝘺 𝘱𝘳𝘰𝘥𝘶𝘤𝘵 𝘯𝘢𝘮𝘦\", after normalize: \"some fancy product name\"\nsample \"▄▀▄▀▄▀ 𝙨𝙤𝙢𝙚 𝙛𝙖𝙣𝙘𝙮 𝙥𝙧𝙤𝙙𝙪𝙘𝙩 𝙣𝙖𝙢𝙚 ▄▀▄▀▄▀\", after normalize: \"some fancy product name\"\nsample \"🌌  🎀 𝚜𝚘𝚖𝚎 𝚏𝚊𝚗𝚌𝚢 𝚙𝚛𝚘𝚍𝚞𝚌𝚝 𝚗𝚊𝚖𝚎🌌  🎀 \", after normalize: \"some fancy product name\"\nsample \"(っ◔◡◔)っ ♥ some fancy product name ♥\", after normalize: \"some fancy product name\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### Data preparation","metadata":{"id":"AEnrIDAxlUTb"}},{"cell_type":"code","source":"stopwords = pd.read_csv('/kaggle/input/data-dl-final/custom_stop_words.csv', header=0, encoding='utf-8')\nstopwords = stopwords['words'].tolist()\nmasked_data = '/kaggle/input/data-dl-final/masked_data.csv'\nmasked = pd.read_csv(masked_data, encoding='utf8')","metadata":{"id":"GjsU07p27s0i","execution":{"iopub.status.busy":"2022-12-12T11:23:33.706512Z","iopub.execute_input":"2022-12-12T11:23:33.706914Z","iopub.status.idle":"2022-12-12T11:23:38.041680Z","shell.execute_reply.started":"2022-12-12T11:23:33.706881Z","shell.execute_reply":"2022-12-12T11:23:38.039940Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"masked = masked.sample(frac=1)\nmasked","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:23:38.043689Z","iopub.execute_input":"2022-12-12T11:23:38.044280Z","iopub.status.idle":"2022-12-12T11:23:38.631883Z","shell.execute_reply.started":"2022-12-12T11:23:38.044229Z","shell.execute_reply":"2022-12-12T11:23:38.630226Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                    sample       label\n340635   nước hoa hồng byphasse lotion tonique face mọi...     Mỹ phẩm\n379540     bộ nước rửa mặt <UNKNUM> thành phần thiên nhiên     Mỹ phẩm\n847011   lucky <UNKNUM> analog stick joystick axis sens...     unknown\n688584   viên uống collagen nature's way beauty collage...     unknown\n939931                              battlefield v <UNKNUM>     unknown\n...                                                    ...         ...\n1031710                                   ak nút bèo trắng  Thời trang\n611916   thanh chắn cửa chắn cầu thang umoo chặn cửa cầ...     Mẹ & Bé\n267741   <UNKNUM> sleeping silk night eye mask cover he...     Mỹ phẩm\n1065189                            quần jogger <UNKNUM> xl  Thời trang\n435272            tẩy da chết white conc nhật bản <UNKNUM>     Mỹ phẩm\n\n[1138950 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>340635</th>\n      <td>nước hoa hồng byphasse lotion tonique face mọi...</td>\n      <td>Mỹ phẩm</td>\n    </tr>\n    <tr>\n      <th>379540</th>\n      <td>bộ nước rửa mặt &lt;UNKNUM&gt; thành phần thiên nhiên</td>\n      <td>Mỹ phẩm</td>\n    </tr>\n    <tr>\n      <th>847011</th>\n      <td>lucky &lt;UNKNUM&gt; analog stick joystick axis sens...</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>688584</th>\n      <td>viên uống collagen nature's way beauty collage...</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>939931</th>\n      <td>battlefield v &lt;UNKNUM&gt;</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1031710</th>\n      <td>ak nút bèo trắng</td>\n      <td>Thời trang</td>\n    </tr>\n    <tr>\n      <th>611916</th>\n      <td>thanh chắn cửa chắn cầu thang umoo chặn cửa cầ...</td>\n      <td>Mẹ &amp; Bé</td>\n    </tr>\n    <tr>\n      <th>267741</th>\n      <td>&lt;UNKNUM&gt; sleeping silk night eye mask cover he...</td>\n      <td>Mỹ phẩm</td>\n    </tr>\n    <tr>\n      <th>1065189</th>\n      <td>quần jogger &lt;UNKNUM&gt; xl</td>\n      <td>Thời trang</td>\n    </tr>\n    <tr>\n      <th>435272</th>\n      <td>tẩy da chết white conc nhật bản &lt;UNKNUM&gt;</td>\n      <td>Mỹ phẩm</td>\n    </tr>\n  </tbody>\n</table>\n<p>1138950 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"masked['sample'] = masked['sample'].str.replace('<UNKNUM>','')\nmasked['sample'] = masked['sample'].str.replace(r'\\s+', ' ', regex=True)\nmasked = masked[~(masked['label'] == 'unknown')]\nmasked.dropna(inplace=True)\nmasked['label'].value_counts()","metadata":{"id":"pQjB1T-tjO4f","outputId":"b17a6b8a-50ee-42f8-eebd-a1e7b2e5217c","execution":{"iopub.status.busy":"2022-12-12T11:23:38.633940Z","iopub.execute_input":"2022-12-12T11:23:38.634443Z","iopub.status.idle":"2022-12-12T11:23:48.014255Z","shell.execute_reply.started":"2022-12-12T11:23:38.634402Z","shell.execute_reply":"2022-12-12T11:23:48.013136Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  return func(*args, **kwargs)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Điện tử - Điện máy    260085\nThời trang            196907\nMỹ phẩm               189248\nMẹ & Bé               177774\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"vectorizer = CountVectorizer(lowercase=False, ngram_range=(1, 3), stop_words=stopwords, analyzer='word',tokenizer =tokenizer, min_df=0.001)\nX = vectorizer.fit_transform(masked['sample'])\nX = X.astype(np.float64)\ny = masked['label']\n\ndef get_name(num):\n    convert = {\n        0: \"Mẹ & Bé\",\n        1: \"Mỹ phẩm\",\n        2: \"Thời trang\",\n        3: \"Điện tử - Điện máy\"\n    }\n    return convert.get(num, 'idk')","metadata":{"id":"-IP4aLvO8JdQ","execution":{"iopub.status.busy":"2022-12-12T11:23:48.015449Z","iopub.execute_input":"2022-12-12T11:23:48.015763Z","iopub.status.idle":"2022-12-12T11:24:27.615725Z","shell.execute_reply.started":"2022-12-12T11:23:48.015734Z","shell.execute_reply":"2022-12-12T11:24:27.614361Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:24:27.617363Z","iopub.execute_input":"2022-12-12T11:24:27.617725Z","iopub.status.idle":"2022-12-12T11:24:27.871102Z","shell.execute_reply.started":"2022-12-12T11:24:27.617693Z","shell.execute_reply":"2022-12-12T11:24:27.869844Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"MODEL1\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel1 = RandomForestClassifier(max_depth=9, random_state=0, n_jobs =-1)\nmodel1.fit(X_train, y_train) \ny_pred = model1.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","metadata":{"id":"lucHV9PWwoTt","outputId":"e136ff70-77d5-47bc-817b-6ce6a824750a","execution":{"iopub.status.busy":"2022-12-12T11:24:27.872628Z","iopub.execute_input":"2022-12-12T11:24:27.873137Z","iopub.status.idle":"2022-12-12T11:24:54.390406Z","shell.execute_reply.started":"2022-12-12T11:24:27.873103Z","shell.execute_reply":"2022-12-12T11:24:54.388689Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[[21145   421  1045 13118]\n [   85 19193   654 17927]\n [ 1062   668 25323 12096]\n [  153   110   277 51526]]\n                    precision    recall  f1-score   support\n\n           Mẹ & Bé       0.94      0.59      0.73     35729\n           Mỹ phẩm       0.94      0.51      0.66     37859\n        Thời trang       0.93      0.65      0.76     39149\nĐiện tử - Điện máy       0.54      0.99      0.70     52066\n\n          accuracy                           0.71    164803\n         macro avg       0.84      0.68      0.71    164803\n      weighted avg       0.81      0.71      0.71    164803\n\n0.7110732207544765\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_pred1 = dtc.predict(X_test)\nprint(confusion_matrix(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))\nprint(accuracy_score(y_test, y_pred1))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:24:54.392661Z","iopub.execute_input":"2022-12-12T11:24:54.393532Z","iopub.status.idle":"2022-12-12T11:29:42.380528Z","shell.execute_reply.started":"2022-12-12T11:24:54.393480Z","shell.execute_reply":"2022-12-12T11:29:42.379342Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[[33803   733   650   543]\n [  898 36031   562   368]\n [  342   246 38470    91]\n [  869   512   347 50338]]\n                    precision    recall  f1-score   support\n\n           Mẹ & Bé       0.94      0.95      0.94     35729\n           Mỹ phẩm       0.96      0.95      0.96     37859\n        Thời trang       0.96      0.98      0.97     39149\nĐiện tử - Điện máy       0.98      0.97      0.97     52066\n\n          accuracy                           0.96    164803\n         macro avg       0.96      0.96      0.96    164803\n      weighted avg       0.96      0.96      0.96    164803\n\n0.9626159717966299\n","output_type":"stream"}]},{"cell_type":"markdown","source":"MODEL2","metadata":{}},{"cell_type":"code","source":"#import tensorflow as tf\n#from keras.models import Model\n#from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Flatten\n#from tensorflow.keras.optimizers import Adam\n#from keras.preprocessing import sequence\n#from tensorflow.keras.utils import to_categorical\n#from keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:29:42.384169Z","iopub.execute_input":"2022-12-12T11:29:42.384523Z","iopub.status.idle":"2022-12-12T11:29:42.390427Z","shell.execute_reply.started":"2022-12-12T11:29:42.384492Z","shell.execute_reply":"2022-12-12T11:29:42.388955Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#def RNN():\n    #inputs = Input(name='inputs',shape=1818)\n    #layer = Embedding(1000,50,input_length=1818)(inputs)\n    #layer = LSTM(64)(layer)\n    #layer = Dense(256,name='FC1')(layer)\n    #layer = Activation('relu')(layer)\n    #layer = Dropout(0.5)(layer)\n    #layer = Dense(1,name='out_layer')(layer)\n    #layer = Activation('sigmoid')(layer)\n    #layer = Flatten()(layer)\n    #model = Model(inputs=inputs,outputs=layer)\n    #return model\n#model2 = RNN()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:29:42.392008Z","iopub.execute_input":"2022-12-12T11:29:42.392449Z","iopub.status.idle":"2022-12-12T11:29:42.404225Z","shell.execute_reply.started":"2022-12-12T11:29:42.392346Z","shell.execute_reply":"2022-12-12T11:29:42.402871Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#adam = tf.keras.optimizers.Adam(learning_rate = 0.001)\n#model2.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = adam)\n#model2.fit(X_train,y_train,batch_size=128,epochs=20,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n#y_pred2 = model2.predict(X_test)\n#print(confusion_matrix(y_test,y_pred2))\n#print(classification_report(y_test,y_pred2))\n#print(accuracy_score(y_test, y_pred2))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:29:42.405738Z","iopub.execute_input":"2022-12-12T11:29:42.406616Z","iopub.status.idle":"2022-12-12T11:29:42.416697Z","shell.execute_reply.started":"2022-12-12T11:29:42.406581Z","shell.execute_reply":"2022-12-12T11:29:42.415708Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB()\nbnb.fit(X_train, y_train)\ny_pred2=bnb.predict(X_test)\nprint(confusion_matrix(y_test,y_pred2))\nprint(classification_report(y_test,y_pred2))\nprint(accuracy_score(y_test, y_pred2))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:29:42.418097Z","iopub.execute_input":"2022-12-12T11:29:42.418548Z","iopub.status.idle":"2022-12-12T11:29:51.568982Z","shell.execute_reply.started":"2022-12-12T11:29:42.418504Z","shell.execute_reply":"2022-12-12T11:29:51.567215Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[[32560  1478  1048   643]\n [  620 36106   758   375]\n [  268    86 38708    87]\n [  564   398   338 50766]]\n                    precision    recall  f1-score   support\n\n           Mẹ & Bé       0.96      0.91      0.93     35729\n           Mỹ phẩm       0.95      0.95      0.95     37859\n        Thời trang       0.95      0.99      0.97     39149\nĐiện tử - Điện máy       0.98      0.98      0.98     52066\n\n          accuracy                           0.96    164803\n         macro avg       0.96      0.96      0.96    164803\n      weighted avg       0.96      0.96      0.96    164803\n\n0.9595699107419161\n","output_type":"stream"}]},{"cell_type":"markdown","source":"MODEL3","metadata":{}},{"cell_type":"code","source":"#from sklearn.naive_bayes import MultinomialNB\n#from sklearn.pipeline import Pipeline\n#from sklearn.feature_extraction.text import TfidfTransformer\n\n#nb = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB())])\n#nb.fit(X_train, y_train)\n\n#%%time\n#from sklearn.metrics import classification_report\n#y_pred3 = nb.predict(X_test)\n#print(confusion_matrix(y_test,y_pred3))\n#print(classification_report(y_test,y_pred3))\n#print(accuracy_score(y_test, y_pred3))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:29:51.571155Z","iopub.execute_input":"2022-12-12T11:29:51.572501Z","iopub.status.idle":"2022-12-12T11:29:51.579773Z","shell.execute_reply.started":"2022-12-12T11:29:51.572440Z","shell.execute_reply":"2022-12-12T11:29:51.578020Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(X_train, y_train)\ny_pred3=spam_detect_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred3))\nprint(classification_report(y_test,y_pred3))\nprint(accuracy_score(y_test, y_pred3))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T11:29:51.581909Z","iopub.execute_input":"2022-12-12T11:29:51.582503Z","iopub.status.idle":"2022-12-12T11:30:00.436957Z","shell.execute_reply.started":"2022-12-12T11:29:51.582449Z","shell.execute_reply":"2022-12-12T11:30:00.435401Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[[33594  1023   270   842]\n [  864 36089   208   698]\n [  425   183 37670   871]\n [  656   320    66 51024]]\n                    precision    recall  f1-score   support\n\n           Mẹ & Bé       0.95      0.94      0.94     35729\n           Mỹ phẩm       0.96      0.95      0.96     37859\n        Thời trang       0.99      0.96      0.97     39149\nĐiện tử - Điện máy       0.95      0.98      0.97     52066\n\n          accuracy                           0.96    164803\n         macro avg       0.96      0.96      0.96    164803\n      weighted avg       0.96      0.96      0.96    164803\n\n0.9610079913593806\n","output_type":"stream"}]},{"cell_type":"markdown","source":"MODEL4","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nclf = SVC(gamma='auto')\nclf.fit(X_train, y_train)\ny_pred4 = clf.predict(X_test)\nprint(confusion_matrix(y_test,y_pred4))\nprint(classification_report(y_test,y_pred4))\nprint(accuracy_score(y_test, y_pred4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MODEL5","metadata":{}},{"cell_type":"code","source":"max_len = np.max(X_train.apply(lambda x :len(x)))\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(1000, 126, input_length=max_len),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=True)),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\nopt = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}