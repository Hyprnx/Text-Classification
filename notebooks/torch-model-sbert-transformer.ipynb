{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hardware Preview","metadata":{}},{"cell_type":"code","source":"!lscpu\n# Expect Intel Xeon Chip @2.0Ghz with 2 cores 4 threads, 2xT4 wont give you this much CPU power, but more GPU power in predicting samples.\n!nvidia-smi \n# nVidia P100 expected, 2xT4 requires setup encode_multi_process, read it here https://www.sbert.net/docs/package_reference/SentenceTransformer.html\n!python -V \n# Python 3.7.12\n!cmake --version\n# cmake version 3.22.5\n!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2022-12-19T12:42:48.835851Z","iopub.execute_input":"2022-12-19T12:42:48.836263Z","iopub.status.idle":"2022-12-19T12:42:53.903875Z","shell.execute_reply.started":"2022-12-19T12:42:48.836180Z","shell.execute_reply":"2022-12-19T12:42:53.902758Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Architecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 48 bits virtual\nCPU(s):                          2\nOn-line CPU(s) list:             0,1\nThread(s) per core:              2\nCore(s) per socket:              1\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           85\nModel name:                      Intel(R) Xeon(R) CPU @ 2.00GHz\nStepping:                        3\nCPU MHz:                         2000.184\nBogoMIPS:                        4000.36\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       32 KiB\nL1i cache:                       32 KiB\nL2 cache:                        1 MiB\nL3 cache:                        38.5 MiB\nNUMA node0 CPU(s):               0,1\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Mitigation; PTE Inversion\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nVulnerability Meltdown:          Mitigation; PTI\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no mic\n                                 rocode; SMT Host state unknown\nVulnerability Retbleed:          Mitigation; IBRS\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled v\n                                 ia prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user\n                                  pointer sanitization\nVulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, RSB filling\n                                 , PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n                                 d sse4_1 sse4_2 x2apic movbe popcnt aes xsave a\n                                 vx f16c rdrand hypervisor lahf_lm abm 3dnowpref\n                                 etch invpcid_single pti ssbd ibrs ibpb stibp fs\n                                 gsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms \n                                 invpcid rtm mpx avx512f avx512dq rdseed adx sma\n                                 p clflushopt clwb avx512cd avx512bw avx512vl xs\n                                 aveopt xsavec xgetbv1 xsaves arat md_clear arch\n                                 _capabilities\nMon Dec 19 12:42:50 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nPython 3.7.12\ncmake version 3.22.5\n\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\nCuda compilation tools, release 11.0, V11.0.221\nBuild cuda_11.0_bu.TC445_37.28845127_0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Install needed library","metadata":{}},{"cell_type":"code","source":"!pip install -q visen\n!pip install -q sentence-transformers\n!pip install -q \"modin[all]\"\n!pip install onnx\n!pip install onnxruntime","metadata":{"id":"Iw28u2cN5rjx","outputId":"e9408877-2e7f-484b-f81f-3313c3c07801","execution":{"iopub.status.busy":"2022-12-19T12:42:53.906209Z","iopub.execute_input":"2022-12-19T12:42:53.907604Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -q pyvi\n# !pip install -q pretty-confusion-matrix\n# !pip install -q Cython\n# !pip uninstall -q -y lightgbm\n# !apt-get -q install -y libboost-all-dev\n# !git clone --quiet --recursive https://github.com/Microsoft/LightGBM\n# !git clone --quiet https://github.com/coccoc/coccoc-tokenizer.git\n\n# !cd coccoc-tokenizer/ && mkdir build\n# !cd coccoc-tokenizer/ && cd build && cmake -DBUILD_PYTHON=1 ..\n# !cd coccoc-tokenizer/ && cd build && make install\n# !cd coccoc-tokenizer/python && python setup.py install\n\n# !cd LightGBM/ && rm -r build\n# !cd LightGBM/ && mkdir build\n# !cd LightGBM/ && cd build && cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\n# !cd LightGBM/ && cd build && make -j$(nproc)\n# !cd LightGBM/python-package/;python setup.py install --precompile\n# !mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n# !rm -r LightGBM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom unicodedata import normalize\nfrom string import punctuation\nimport visen\nimport pickle\n\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport scipy\nimport sys\nimport os\nfrom time import time\n\nos.environ[\"MODIN_ENGINE\"] = \"dask\"\nos.environ[\"MODIN_CPUS\"] = \"16\"\n# import pandas as pd\nimport modin.pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nfrom torch import optim\n\n# from CocCocTokenizer import PyTokenizer\n# tokenizer = PyTokenizer(load_nontone_data=True).word_tokenize\n# import lightgbm as lgb\n# from pretty_confusion_matrix import pp_matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opening_ls = ['[', '{', '‚ÅÖ', '‚å©', '‚é°', '‚é¢', '‚é£', '‚éß', '‚é®', '‚é©', '‚ù¨', '‚ù∞', '‚ù≤', '‚ù¥', '‚ü¶', '‚ü®', '‚ü™', '‚ü¨', '‚¶É', '‚¶á', '‚¶â',\n              '‚¶ã', '‚¶ç', '‚¶è', '‚¶ë', '‚¶ì', '‚¶ï', '‚¶ó', '‚ßº', '‚∏Ç', '‚∏Ñ', '‚∏â', '‚∏å', '‚∏ú', '‚∏¢', '‚∏§', '‚∏¶', '„Äà', '„Ää', '„Äå', '„Äé',\n              '„Äê', '„Äî', '„Äñ', '„Äò', '„Äö', 'Ôπõ', 'Ôπù', 'Ôºª', 'ÔΩõ', 'ÔΩ¢', 'ÔΩ£']\n\nclosing_ls = [']', '}', '‚ÅÜ', '‚å™', '‚é§', '‚é•', '‚é¶', '‚é´', '‚é¨', '‚é≠', '‚ù≠', '‚ù±', '‚ù≥', '‚ùµ', '‚üß', '‚ü©', '‚ü´', '‚ü≠', '‚¶Ñ', '‚¶à', '‚¶ä',\n              '‚¶å', '‚¶é', '‚¶ê', '‚¶í', '‚¶î', '‚¶ñ', '‚¶ò', '‚ßΩ', '‚∏É', '‚∏Ö', '‚∏ä', '‚∏ç', '‚∏ù', '‚∏£', '‚∏•', '‚∏ß', '„Äâ', '„Äã', '„Äç', '„Äè',\n              '„Äë', '„Äï', '„Äó', '„Äô', '„Äõ', 'Ôπú', 'Ôπû', 'ÔºΩ', 'ÔΩù', 'ÔΩ£']\n\nopening_bracket = {key: '(' for key in opening_ls}\nclosing_bracket = {key: ')' for key in closing_ls}\n\nopening_bracket_pattern = {f\"\\\\{key}\": \"(\" for key in opening_ls}\nclosing_bracket_pattern = {f\"\\\\{key}\": \")\" for key in closing_ls}\n\n### constant\nPUNC = '!\\\"#$&()*+,-‚Äì‚àí./:;=?@[\\]^_`{|}~‚Äù‚Äú`¬∞¬≤Àà‚Äê„Ñß‚Äõ‚àº‚Äô'  # remove <> for number_sym and unknown_sym\nre_num_and_decimal = '[0-9]*[,.\\-]*[0-9]*[,.\\-]*[0-9]*[.,\\-]*[0-9]*[,.\\-]*[0-9]+[.,]?'\nre_unknown = '[a-z]+[\\d]+[\\w]*|[\\d]+[a-z]+[\\w]*'\nre_vnese_txt = r'[^a-z0-9A-Z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ√Ω√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√çƒ®·ªà·ªä√í√ì√ï·ªå·ªé√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö≈®·ª§·ª¶∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤·ª¥·ª∂·ª∏√ù\\s|_]'\nspecial_punc = {'‚Äù': '\"', '': '', \"‚Äô\": \"'\", \"`\": \"'\"}\n\n\ndef sizeof_fmt(num, suffix='B'):\n    '''\n    Utility function to quickly view memory usage\n    :arg num: number of bytes\n    :arg suffix: suffix of the number\n    :return: string of the number with suffix\n\n    by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified\n    '''\n    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\ndef replace_all(replacer: dict, txt: str) -> str:\n    \"\"\"\n    Replace all the keys in the dictionary with their respective values.\n    :param replacer: dictionary of keys are the words to be replaced and values are the words to replace them with\n    :param txt: subject string\n    :return: string after replacement\n    \"\"\"\n    for old, new in replacer.items():\n        txt = txt.replace(old, new)\n    return txt\n\n\ndef replace_num(txt: str) -> str:\n    \"\"\"\n    Replace all the numbers in the text with blank\n    :param text: subject string\n    :return: string after replacement\n    \"\"\"\n    text = re.sub(re_num_and_decimal, '', txt)\n    return text\n\n\ndef replace_unknown(text: str) -> str:\n    \"\"\"\n    Replace all the predefined unknown symbols in the text with blank\n    :param text: subject string\n    :return: string after replacement\n    \"\"\"\n    text = re.sub(re_unknown, '', text)\n    return text\n\n\ndef unicode_normalizer(text, forms: list = ['NFKC', 'NKFD', 'NFC', 'NFD']) -> str:\n    \"\"\"\n    Normalize unicode text\n    :param text: subject string\n    :param forms: unicode normalization forms\n    :return: string after normalization\n    \"\"\"\n    for form in forms:\n        text = normalize(form, text)\n    return text\n\n\ndef normalize_bracket(text: str) -> str:\n    \"\"\"\n    Normalize brackets in the text with predefined string for later use\n    :param text: subject string\n    :return: transformed string\n    \"\"\"\n    text = replace_all(opening_bracket, text)\n    text = replace_all(closing_bracket, text)\n    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n    return text\n\n\ndef remove_punc(text: str) -> str:\n    \"\"\"\n    Remove punctuations in the text\n    :param text: subject string\n    :return: string after removal\n    \"\"\"\n    r = re.compile(r'[\\s{}]+'.format(re.escape(PUNC)))\n    text = r.split(text)\n    return ' '.join(i for i in text if i)\n\n\ndef norm(text: str) -> str:\n    \"\"\"\n    Normalize text by removing punctuations, numbers, unknown symbols, brackets, and normalize unicode\n    :param text: subject string\n    :return: normalized string\n    \"\"\"\n    text = str(text)\n    text = text.lower()\n    text = text.split('\\n')[0]\n    text = unicode_normalizer(text, [\"NFKC\", \"NFKD\", \"NFD\", \"NFC\"])\n    text = replace_all(special_punc, text)\n    text = normalize_bracket(text)\n    text = replace_unknown(text)\n    text = replace_num(text)\n    text = remove_punc(text)\n    text = re.sub(re_vnese_txt, \"\", text)\n    text = text.strip()\n    return visen.clean_tone(text)","metadata":{"id":"phUxD5yp5sk4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_txts = [\n    '„Äñùî∞ùî¨ùî™ùî¢ ùî£ùîûùî´ùî†ùî∂ ùî≠ùîØùî¨ùî°ùî≤ùî†ùî± ùî´ùîûùî™ùî¢„Äó',\n    'ùñòùñîùñíùñä ùñãùñÜùñìùñàùñû ùñïùñóùñîùñâùñöùñàùñô ùñìùñÜùñíùñä',\n    '‚ù∞some product code which will overfit this sample‚ù± ùìºùì∏ùì∂ùìÆ ùìØùì™ùì∑ùì¨ùîÇ ùìπùìªùì∏ùì≠ùìæùì¨ùìΩ ùì∑ùì™ùì∂ùìÆ',\n    'ùìàùëúùìÇùëí ùíªùí∂ùìÉùí∏ùìé ùìÖùìáùëúùíπùìäùí∏ùìâ ùìÉùí∂ùìÇùëí',\n    '{ùï§ùï†ùïûùïñ ùïóùïíùïüùïîùï™ ùï°ùï£ùï†ùïïùï¶ùïîùï• ùïüùïíùïûùïñ',\n    '‚òØüòùÔΩìÔΩèÔΩçÔΩÖ ÔΩÜÔΩÅÔΩéÔΩÉÔΩô ÔΩêÔΩíÔΩèÔΩÑÔΩïÔΩÉÔΩî ÔΩéÔΩÅÔΩçÔΩÖ‚òØüòù',\n    'ùê¨ùê®ùê¶ùêû ùêüùêöùêßùêúùê≤ ùê©ùê´ùê®ùêùùêÆùêúùê≠ ùêßùêöùê¶ùêû',\n    'ùòÄùóºùó∫ùó≤ ùó≥ùóÆùóªùó∞ùòÜ ùóΩùóøùóºùó±ùòÇùó∞ùòÅ ùóªùóÆùó∫ùó≤',\n    'ùò¥ùò∞ùòÆùò¶ ùòßùò¢ùòØùò§ùò∫ ùò±ùò≥ùò∞ùò•ùò∂ùò§ùòµ ùòØùò¢ùòÆùò¶',\n    '‚ñÑ‚ñÄ‚ñÑ‚ñÄ‚ñÑ‚ñÄ ùô®ùô§ùô¢ùôö ùôõùôñùô£ùôòùôÆ ùô•ùôßùô§ùôôùô™ùôòùô© ùô£ùôñùô¢ùôö ‚ñÑ‚ñÄ‚ñÑ‚ñÄ‚ñÑ‚ñÄ',\n    'üåå  üéÄ ùöúùöòùöñùöé ùöèùöäùöóùöåùö¢ ùöôùöõùöòùöçùöûùöåùöù ùöóùöäùöñùöéüåå  üéÄ ',\n    '(„Å£‚óî‚ó°‚óî)„Å£ ‚ô• some fancy product name ‚ô•',\n]\n\nfor sample in sample_txts:\n    print(f'sample \\\"{sample}\\\", after normalize: \\\"{norm(sample)}\\\"')","metadata":{"id":"X4uDz_r_5wRq","outputId":"366d9f61-2469-49d4-af3d-8fe152d9ef6d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still normalize function but for DataFrame","metadata":{}},{"cell_type":"code","source":"def dataframe_normalize(df: pd.DataFrame=None) -> pd.DataFrame:\n    \"\"\"\n    Normalize dataframe\n    :param df: Dataframe to normalize\n    :return: Dataframe normalized\n    \"\"\"    \n    assert 'sample' in df.columns, f\"DataFrame with column name 'sample' expected, got {df.columns} \"\n    begin = time()\n\n    df['sample'] = df['sample'].str.lower()\n    df['sample'] = df['sample'].str.replace('[a-z]+[\\d]+[\\w]*|[\\d]+[a-z]+[\\w]*', '', regex=True) # replace all product code with blank\n    df['sample'] = df['sample'].str.replace('[0-9]*[,.\\-]*[0-9]*[,.\\-]*[0-9]*[.,\\-]*[0-9]*[,.\\-]*[0-9]+[.,]?', '', regex=True) # replace all number with blank\n    df['sample'] = df['sample'].replace(opening_bracket_pattern, regex=True) # normalize opening brackets, listed above\n    df['sample'] = df['sample'].replace(closing_bracket_pattern, regex=True) # normalize closing brackets, listed above\n    df['sample'] = df['sample'].str.replace(\"[\\(\\[].*?[\\)\\]]\", '', regex=True) # remove all content inside brackets, eg: (Si√™u sale 12/12)\n    df['sample'] = df['sample'].str.normalize('NFKC') # Normalize unicode with NFKC standard \n    df['sample'] = df['sample'].str.normalize('NFKD')\n    df['sample'] = df['sample'].str.normalize('NFC')\n    df['sample'] = df['sample'].str.normalize('NFD')\n#   # Remove non Vietnamese text, seems like unessesary right now, might consider remove later to improve run time.\n    df['sample'] = df['sample'].str.replace(r\"[^a-z0-9A-Z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ√Ω√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√çƒ®·ªà·ªä√í√ì√ï·ªå·ªé√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö≈®·ª§·ª¶∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤·ª¥·ª∂·ª∏√ù\\s|_]\", '', regex=True)\n    df['sample'] = df['sample'].str.replace('ƒë', 'd').str.strip() # Special case where unicode normalizer does not consider 'ƒë' a combination of base unicode characters\n    print('Total enlapsed time:', time()- begin, 'seconds')\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Data preparation","metadata":{"id":"AEnrIDAxlUTb"}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/data-dl-final/shopee_data.csv', encoding='utf8', sep='\\t')","metadata":{"id":"GjsU07p27s0i","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.sample(frac=1)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dataframe_normalize(data)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[~(data['label'] == 'unknown')]\ndata.dropna(inplace=True)\ndata['label'].value_counts()","metadata":{"id":"pQjB1T-tjO4f","outputId":"b17a6b8a-50ee-42f8-eebd-a1e7b2e5217c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train , y_val = train_test_split(data['sample'], data['label'],train_size=0.7, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ny_train = torch.tensor(le.fit_transform(y_train.values)).cuda()\ny_val = torch.tensor(le.transform(y_val.values)).cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nembedder = SentenceTransformer('keepitreal/vietnamese-sbert',device='cuda')\n\nX_train = X_train.tolist()\nX_train = embedder.encode(X_train, batch_size=2048)\nX_train = torch.from_numpy(X_train).float()\nX_train = X_train.cuda()\n\n\nX_val = X_val.tolist()\nX_val = embedder.encode(X_val, batch_size=2048)\nX_val = torch.from_numpy(X_val).float()\nX_val = X_val.cuda()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(le.inverse_transform([0]))\nprint(le.inverse_transform([1]))\nprint(le.inverse_transform([2]))\nprint(le.inverse_transform([3]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# THIS MODEL IS **EXTREMELY** SIMPLE, TRY TO BUILD A **BIT** MORE COMPLEX MODEL!","metadata":{}},{"cell_type":"code","source":"from torch import nn\nmodel = nn.Sequential(nn.Linear(768, 64),\n                      nn.ReLU(),\n                      nn.Linear(64, 64),\n                      nn.ReLU(),\n                      nn.Dropout(0.1),\n                      nn.Linear(64, data['label'].nunique()),\n                      nn.LogSoftmax(dim=1))\n\n# transfer model to gpu for training acceleration\nmodel = model.to('cuda')\n\n# Negative Log-Likelihood Loss\ncriterion = nn.NLLLoss()\n\n# Forward pass, get our logits\nlogps = model(X_train)\n\n# Calculate the loss with the logits and the labels\nloss = criterion(logps, y_train)\nloss.backward()\n\n# Optimizers require the parameters to optimize and a learning rate\noptimizer = optim.Adam(model.parameters(), lr=0.002)\n\nnext(model.parameters()).is_cuda","metadata":{"id":"lucHV9PWwoTt","outputId":"e136ff70-77d5-47bc-817b-6ce6a824750a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1/0\n# Prevent below blocks from running when excute run all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),key= lambda x: -x[1])[:20]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\nepochs = 200\nfor e in range(epochs):\n    optimizer.zero_grad()\n    output = model.forward(X_train)\n    loss = criterion(output, y_train)\n    loss.backward()\n    train_loss = loss.item()\n    train_losses.append(train_loss)\n    \n    # Sample evaluation step:\n    with torch.no_grad():\n        log_ps = model(X_val)\n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim=1)\n        equals = top_class == y_val.view(*top_class.shape)\n        val_accuracy = torch.mean(equals.float())\n    \n    optimizer.step()\n    model.train()\n    \n    if e % 5 == 0:\n        print(f\"Epoch: {e+1}/{epochs}.. \",\n              f\"Training Loss: {train_loss:.3f}.. \",\n              f\"Training Acc: {val_accuracy:.3f}.. \")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_sample = ['iphone 13 promax', 'ph·∫•n ph·ªß h·ªìng ƒë·∫•t', '√°o kho√°c d·∫°']\n\ndef predict(model, batch, output = 'df', mask_inference=True):\n    assert output in ['df', 'dict', 'list']\n    batch_sample = [norm(i) for i in batch]\n    if mask_inference:\n        print(batch_sample)\n    \n    batch_sample = embedder.encode(batch_sample, batch_size = min(len(batch), 2048))\n    batch_sample = torch.from_numpy(batch_sample).float().cuda()\n    model.eval()\n    with torch.no_grad():\n        out_data = model(batch_sample)\n        ps = torch.exp(out_data)\n        pred = ps.max(1).indices.cpu().numpy()\n        res = [le.inverse_transform([i])[0] for i in pred]\n    \n    if output == 'list':\n        return res\n    \n    data_tuple = zip(batch, res)\n    \n    if output == 'df':\n        return pd.DataFrame(data_tuple, columns=['name', 'prediction'])\n    \n    if output == 'dict':\n        return dict(data_tuple)\n\n\npredict(model, new_sample, output='dict', mask_inference=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/data-dl-final/test_DTDM.csv', sep=',', encoding='utf8', on_bad_lines='skip')\ntest_sample = df.name.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\n\nres = predict(model, test_sample, mask_inference=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = predict(model, test_sample, mask_inference=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res['prediction'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wrong_sample = res[res['prediction'] != 'ƒêi·ªán t·ª≠ - ƒêi·ªán m√°y']['name'].to_list()\nwrong_sample\n\nif wrong_sample:\n    print(predict(model, wrong_sample, output = 'dict'))\nwrong_sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wrong_sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random = ['√°o cho√†ng ƒë√¥ng']\npredict(model,random , output = 'dict')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.state_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'classifier.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('label_encoder.pkl', 'wb') as encoder:\n    pickle.dump(le, encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}