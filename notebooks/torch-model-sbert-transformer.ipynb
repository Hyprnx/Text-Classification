{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hardware Preview","metadata":{}},{"cell_type":"code","source":"!lscpu\n# Expect Intel Xeon Chip @2.0Ghz with 2 cores 4 threads, 2xT4 wont give you this much CPU power, but more GPU power in predicting samples.\n!nvidia-smi \n# nVidia P100 expected, 2xT4 requires setup encode_multi_process, read it here https://www.sbert.net/docs/package_reference/SentenceTransformer.html\n!python -V \n# Python 3.7.12\n!cmake --version\n# cmake version 3.22.5\n!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2022-12-19T12:42:48.835851Z","iopub.execute_input":"2022-12-19T12:42:48.836263Z","iopub.status.idle":"2022-12-19T12:42:53.903875Z","shell.execute_reply.started":"2022-12-19T12:42:48.836180Z","shell.execute_reply":"2022-12-19T12:42:53.902758Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Architecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 48 bits virtual\nCPU(s):                          2\nOn-line CPU(s) list:             0,1\nThread(s) per core:              2\nCore(s) per socket:              1\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           85\nModel name:                      Intel(R) Xeon(R) CPU @ 2.00GHz\nStepping:                        3\nCPU MHz:                         2000.184\nBogoMIPS:                        4000.36\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       32 KiB\nL1i cache:                       32 KiB\nL2 cache:                        1 MiB\nL3 cache:                        38.5 MiB\nNUMA node0 CPU(s):               0,1\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Mitigation; PTE Inversion\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nVulnerability Meltdown:          Mitigation; PTI\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no mic\n                                 rocode; SMT Host state unknown\nVulnerability Retbleed:          Mitigation; IBRS\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled v\n                                 ia prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user\n                                  pointer sanitization\nVulnerability Spectre v2:        Mitigation; IBRS, IBPB conditional, RSB filling\n                                 , PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state u\n                                 nknown\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n                                 d sse4_1 sse4_2 x2apic movbe popcnt aes xsave a\n                                 vx f16c rdrand hypervisor lahf_lm abm 3dnowpref\n                                 etch invpcid_single pti ssbd ibrs ibpb stibp fs\n                                 gsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms \n                                 invpcid rtm mpx avx512f avx512dq rdseed adx sma\n                                 p clflushopt clwb avx512cd avx512bw avx512vl xs\n                                 aveopt xsavec xgetbv1 xsaves arat md_clear arch\n                                 _capabilities\nMon Dec 19 12:42:50 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nPython 3.7.12\ncmake version 3.22.5\n\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\nCuda compilation tools, release 11.0, V11.0.221\nBuild cuda_11.0_bu.TC445_37.28845127_0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Install needed library","metadata":{}},{"cell_type":"code","source":"!pip install -q visen\n!pip install -q sentence-transformers\n!pip install -q \"modin[all]\"\n!pip install onnx\n!pip install onnxruntime","metadata":{"id":"Iw28u2cN5rjx","outputId":"e9408877-2e7f-484b-f81f-3313c3c07801","execution":{"iopub.status.busy":"2022-12-19T12:42:53.906209Z","iopub.execute_input":"2022-12-19T12:42:53.907604Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -q pyvi\n# !pip install -q pretty-confusion-matrix\n# !pip install -q Cython\n# !pip uninstall -q -y lightgbm\n# !apt-get -q install -y libboost-all-dev\n# !git clone --quiet --recursive https://github.com/Microsoft/LightGBM\n# !git clone --quiet https://github.com/coccoc/coccoc-tokenizer.git\n\n# !cd coccoc-tokenizer/ && mkdir build\n# !cd coccoc-tokenizer/ && cd build && cmake -DBUILD_PYTHON=1 ..\n# !cd coccoc-tokenizer/ && cd build && make install\n# !cd coccoc-tokenizer/python && python setup.py install\n\n# !cd LightGBM/ && rm -r build\n# !cd LightGBM/ && mkdir build\n# !cd LightGBM/ && cd build && cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\n# !cd LightGBM/ && cd build && make -j$(nproc)\n# !cd LightGBM/python-package/;python setup.py install --precompile\n# !mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n# !rm -r LightGBM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom unicodedata import normalize\nfrom string import punctuation\nimport visen\nimport pickle\n\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport scipy\nimport sys\nimport os\nfrom time import time\n\nos.environ[\"MODIN_ENGINE\"] = \"dask\"\nos.environ[\"MODIN_CPUS\"] = \"16\"\n# import pandas as pd\nimport modin.pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nfrom torch import optim\n\n# from CocCocTokenizer import PyTokenizer\n# tokenizer = PyTokenizer(load_nontone_data=True).word_tokenize\n# import lightgbm as lgb\n# from pretty_confusion_matrix import pp_matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opening_ls = ['[', '{', '⁅', '〈', '⎡', '⎢', '⎣', '⎧', '⎨', '⎩', '❬', '❰', '❲', '❴', '⟦', '⟨', '⟪', '⟬', '⦃', '⦇', '⦉',\n              '⦋', '⦍', '⦏', '⦑', '⦓', '⦕', '⦗', '⧼', '⸂', '⸄', '⸉', '⸌', '⸜', '⸢', '⸤', '⸦', '〈', '《', '「', '『',\n              '【', '〔', '〖', '〘', '〚', '﹛', '﹝', '［', '｛', '｢', '｣']\n\nclosing_ls = [']', '}', '⁆', '〉', '⎤', '⎥', '⎦', '⎫', '⎬', '⎭', '❭', '❱', '❳', '❵', '⟧', '⟩', '⟫', '⟭', '⦄', '⦈', '⦊',\n              '⦌', '⦎', '⦐', '⦒', '⦔', '⦖', '⦘', '⧽', '⸃', '⸅', '⸊', '⸍', '⸝', '⸣', '⸥', '⸧', '〉', '》', '」', '』',\n              '】', '〕', '〗', '〙', '〛', '﹜', '﹞', '］', '｝', '｣']\n\nopening_bracket = {key: '(' for key in opening_ls}\nclosing_bracket = {key: ')' for key in closing_ls}\n\nopening_bracket_pattern = {f\"\\\\{key}\": \"(\" for key in opening_ls}\nclosing_bracket_pattern = {f\"\\\\{key}\": \")\" for key in closing_ls}\n\n### constant\nPUNC = '!\\\"#$&()*+,-–−./:;=?@[\\]^_`{|}~”“`°²ˈ‐ㄧ‛∼’'  # remove <> for number_sym and unknown_sym\nre_num_and_decimal = '[0-9]*[,.\\-]*[0-9]*[,.\\-]*[0-9]*[.,\\-]*[0-9]*[,.\\-]*[0-9]+[.,]?'\nre_unknown = '[a-z]+[\\d]+[\\w]*|[\\d]+[a-z]+[\\w]*'\nre_vnese_txt = r'[^a-z0-9A-ZàáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹýÀÁÃẠẢĂẮẰẲẴẶÂẤẦẨẪẬÈÉẸẺẼÊỀẾỂỄỆĐÌÍĨỈỊÒÓÕỌỎÔỐỒỔỖỘƠỚỜỞỠỢÙÚŨỤỦƯỨỪỬỮỰỲỴỶỸÝ\\s|_]'\nspecial_punc = {'”': '\"', '': '', \"’\": \"'\", \"`\": \"'\"}\n\n\ndef sizeof_fmt(num, suffix='B'):\n    '''\n    Utility function to quickly view memory usage\n    :arg num: number of bytes\n    :arg suffix: suffix of the number\n    :return: string of the number with suffix\n\n    by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified\n    '''\n    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\ndef replace_all(replacer: dict, txt: str) -> str:\n    \"\"\"\n    Replace all the keys in the dictionary with their respective values.\n    :param replacer: dictionary of keys are the words to be replaced and values are the words to replace them with\n    :param txt: subject string\n    :return: string after replacement\n    \"\"\"\n    for old, new in replacer.items():\n        txt = txt.replace(old, new)\n    return txt\n\n\ndef replace_num(txt: str) -> str:\n    \"\"\"\n    Replace all the numbers in the text with blank\n    :param text: subject string\n    :return: string after replacement\n    \"\"\"\n    text = re.sub(re_num_and_decimal, '', txt)\n    return text\n\n\ndef replace_unknown(text: str) -> str:\n    \"\"\"\n    Replace all the predefined unknown symbols in the text with blank\n    :param text: subject string\n    :return: string after replacement\n    \"\"\"\n    text = re.sub(re_unknown, '', text)\n    return text\n\n\ndef unicode_normalizer(text, forms: list = ['NFKC', 'NKFD', 'NFC', 'NFD']) -> str:\n    \"\"\"\n    Normalize unicode text\n    :param text: subject string\n    :param forms: unicode normalization forms\n    :return: string after normalization\n    \"\"\"\n    for form in forms:\n        text = normalize(form, text)\n    return text\n\n\ndef normalize_bracket(text: str) -> str:\n    \"\"\"\n    Normalize brackets in the text with predefined string for later use\n    :param text: subject string\n    :return: transformed string\n    \"\"\"\n    text = replace_all(opening_bracket, text)\n    text = replace_all(closing_bracket, text)\n    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n    return text\n\n\ndef remove_punc(text: str) -> str:\n    \"\"\"\n    Remove punctuations in the text\n    :param text: subject string\n    :return: string after removal\n    \"\"\"\n    r = re.compile(r'[\\s{}]+'.format(re.escape(PUNC)))\n    text = r.split(text)\n    return ' '.join(i for i in text if i)\n\n\ndef norm(text: str) -> str:\n    \"\"\"\n    Normalize text by removing punctuations, numbers, unknown symbols, brackets, and normalize unicode\n    :param text: subject string\n    :return: normalized string\n    \"\"\"\n    text = str(text)\n    text = text.lower()\n    text = text.split('\\n')[0]\n    text = unicode_normalizer(text, [\"NFKC\", \"NFKD\", \"NFD\", \"NFC\"])\n    text = replace_all(special_punc, text)\n    text = normalize_bracket(text)\n    text = replace_unknown(text)\n    text = replace_num(text)\n    text = remove_punc(text)\n    text = re.sub(re_vnese_txt, \"\", text)\n    text = text.strip()\n    return visen.clean_tone(text)","metadata":{"id":"phUxD5yp5sk4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_txts = [\n    '〖𝔰𝔬𝔪𝔢 𝔣𝔞𝔫𝔠𝔶 𝔭𝔯𝔬𝔡𝔲𝔠𝔱 𝔫𝔞𝔪𝔢〗',\n    '𝖘𝖔𝖒𝖊 𝖋𝖆𝖓𝖈𝖞 𝖕𝖗𝖔𝖉𝖚𝖈𝖙 𝖓𝖆𝖒𝖊',\n    '❰some product code which will overfit this sample❱ 𝓼𝓸𝓶𝓮 𝓯𝓪𝓷𝓬𝔂 𝓹𝓻𝓸𝓭𝓾𝓬𝓽 𝓷𝓪𝓶𝓮',\n    '𝓈𝑜𝓂𝑒 𝒻𝒶𝓃𝒸𝓎 𝓅𝓇𝑜𝒹𝓊𝒸𝓉 𝓃𝒶𝓂𝑒',\n    '{𝕤𝕠𝕞𝕖 𝕗𝕒𝕟𝕔𝕪 𝕡𝕣𝕠𝕕𝕦𝕔𝕥 𝕟𝕒𝕞𝕖',\n    '☯😝ｓｏｍｅ ｆａｎｃｙ ｐｒｏｄｕｃｔ ｎａｍｅ☯😝',\n    '𝐬𝐨𝐦𝐞 𝐟𝐚𝐧𝐜𝐲 𝐩𝐫𝐨𝐝𝐮𝐜𝐭 𝐧𝐚𝐦𝐞',\n    '𝘀𝗼𝗺𝗲 𝗳𝗮𝗻𝗰𝘆 𝗽𝗿𝗼𝗱𝘂𝗰𝘁 𝗻𝗮𝗺𝗲',\n    '𝘴𝘰𝘮𝘦 𝘧𝘢𝘯𝘤𝘺 𝘱𝘳𝘰𝘥𝘶𝘤𝘵 𝘯𝘢𝘮𝘦',\n    '▄▀▄▀▄▀ 𝙨𝙤𝙢𝙚 𝙛𝙖𝙣𝙘𝙮 𝙥𝙧𝙤𝙙𝙪𝙘𝙩 𝙣𝙖𝙢𝙚 ▄▀▄▀▄▀',\n    '🌌  🎀 𝚜𝚘𝚖𝚎 𝚏𝚊𝚗𝚌𝚢 𝚙𝚛𝚘𝚍𝚞𝚌𝚝 𝚗𝚊𝚖𝚎🌌  🎀 ',\n    '(っ◔◡◔)っ ♥ some fancy product name ♥',\n]\n\nfor sample in sample_txts:\n    print(f'sample \\\"{sample}\\\", after normalize: \\\"{norm(sample)}\\\"')","metadata":{"id":"X4uDz_r_5wRq","outputId":"366d9f61-2469-49d4-af3d-8fe152d9ef6d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still normalize function but for DataFrame","metadata":{}},{"cell_type":"code","source":"def dataframe_normalize(df: pd.DataFrame=None) -> pd.DataFrame:\n    \"\"\"\n    Normalize dataframe\n    :param df: Dataframe to normalize\n    :return: Dataframe normalized\n    \"\"\"    \n    assert 'sample' in df.columns, f\"DataFrame with column name 'sample' expected, got {df.columns} \"\n    begin = time()\n\n    df['sample'] = df['sample'].str.lower()\n    df['sample'] = df['sample'].str.replace('[a-z]+[\\d]+[\\w]*|[\\d]+[a-z]+[\\w]*', '', regex=True) # replace all product code with blank\n    df['sample'] = df['sample'].str.replace('[0-9]*[,.\\-]*[0-9]*[,.\\-]*[0-9]*[.,\\-]*[0-9]*[,.\\-]*[0-9]+[.,]?', '', regex=True) # replace all number with blank\n    df['sample'] = df['sample'].replace(opening_bracket_pattern, regex=True) # normalize opening brackets, listed above\n    df['sample'] = df['sample'].replace(closing_bracket_pattern, regex=True) # normalize closing brackets, listed above\n    df['sample'] = df['sample'].str.replace(\"[\\(\\[].*?[\\)\\]]\", '', regex=True) # remove all content inside brackets, eg: (Siêu sale 12/12)\n    df['sample'] = df['sample'].str.normalize('NFKC') # Normalize unicode with NFKC standard \n    df['sample'] = df['sample'].str.normalize('NFKD')\n    df['sample'] = df['sample'].str.normalize('NFC')\n    df['sample'] = df['sample'].str.normalize('NFD')\n#   # Remove non Vietnamese text, seems like unessesary right now, might consider remove later to improve run time.\n    df['sample'] = df['sample'].str.replace(r\"[^a-z0-9A-ZàáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹýÀÁÃẠẢĂẮẰẲẴẶÂẤẦẨẪẬÈÉẸẺẼÊỀẾỂỄỆĐÌÍĨỈỊÒÓÕỌỎÔỐỒỔỖỘƠỚỜỞỠỢÙÚŨỤỦƯỨỪỬỮỰỲỴỶỸÝ\\s|_]\", '', regex=True)\n    df['sample'] = df['sample'].str.replace('đ', 'd').str.strip() # Special case where unicode normalizer does not consider 'đ' a combination of base unicode characters\n    print('Total enlapsed time:', time()- begin, 'seconds')\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Data preparation","metadata":{"id":"AEnrIDAxlUTb"}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/data-dl-final/shopee_data.csv', encoding='utf8', sep='\\t')","metadata":{"id":"GjsU07p27s0i","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.sample(frac=1)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dataframe_normalize(data)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[~(data['label'] == 'unknown')]\ndata.dropna(inplace=True)\ndata['label'].value_counts()","metadata":{"id":"pQjB1T-tjO4f","outputId":"b17a6b8a-50ee-42f8-eebd-a1e7b2e5217c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train , y_val = train_test_split(data['sample'], data['label'],train_size=0.7, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ny_train = torch.tensor(le.fit_transform(y_train.values)).cuda()\ny_val = torch.tensor(le.transform(y_val.values)).cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nembedder = SentenceTransformer('keepitreal/vietnamese-sbert',device='cuda')\n\nX_train = X_train.tolist()\nX_train = embedder.encode(X_train, batch_size=2048)\nX_train = torch.from_numpy(X_train).float()\nX_train = X_train.cuda()\n\n\nX_val = X_val.tolist()\nX_val = embedder.encode(X_val, batch_size=2048)\nX_val = torch.from_numpy(X_val).float()\nX_val = X_val.cuda()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(le.inverse_transform([0]))\nprint(le.inverse_transform([1]))\nprint(le.inverse_transform([2]))\nprint(le.inverse_transform([3]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# THIS MODEL IS **EXTREMELY** SIMPLE, TRY TO BUILD A **BIT** MORE COMPLEX MODEL!","metadata":{}},{"cell_type":"code","source":"from torch import nn\nmodel = nn.Sequential(nn.Linear(768, 64),\n                      nn.ReLU(),\n                      nn.Linear(64, 64),\n                      nn.ReLU(),\n                      nn.Dropout(0.1),\n                      nn.Linear(64, data['label'].nunique()),\n                      nn.LogSoftmax(dim=1))\n\n# transfer model to gpu for training acceleration\nmodel = model.to('cuda')\n\n# Negative Log-Likelihood Loss\ncriterion = nn.NLLLoss()\n\n# Forward pass, get our logits\nlogps = model(X_train)\n\n# Calculate the loss with the logits and the labels\nloss = criterion(logps, y_train)\nloss.backward()\n\n# Optimizers require the parameters to optimize and a learning rate\noptimizer = optim.Adam(model.parameters(), lr=0.002)\n\nnext(model.parameters()).is_cuda","metadata":{"id":"lucHV9PWwoTt","outputId":"e136ff70-77d5-47bc-817b-6ce6a824750a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1/0\n# Prevent below blocks from running when excute run all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),key= lambda x: -x[1])[:20]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\nepochs = 200\nfor e in range(epochs):\n    optimizer.zero_grad()\n    output = model.forward(X_train)\n    loss = criterion(output, y_train)\n    loss.backward()\n    train_loss = loss.item()\n    train_losses.append(train_loss)\n    \n    # Sample evaluation step:\n    with torch.no_grad():\n        log_ps = model(X_val)\n        ps = torch.exp(log_ps)\n        top_p, top_class = ps.topk(1, dim=1)\n        equals = top_class == y_val.view(*top_class.shape)\n        val_accuracy = torch.mean(equals.float())\n    \n    optimizer.step()\n    model.train()\n    \n    if e % 5 == 0:\n        print(f\"Epoch: {e+1}/{epochs}.. \",\n              f\"Training Loss: {train_loss:.3f}.. \",\n              f\"Training Acc: {val_accuracy:.3f}.. \")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_sample = ['iphone 13 promax', 'phấn phủ hồng đất', 'áo khoác dạ']\n\ndef predict(model, batch, output = 'df', mask_inference=True):\n    assert output in ['df', 'dict', 'list']\n    batch_sample = [norm(i) for i in batch]\n    if mask_inference:\n        print(batch_sample)\n    \n    batch_sample = embedder.encode(batch_sample, batch_size = min(len(batch), 2048))\n    batch_sample = torch.from_numpy(batch_sample).float().cuda()\n    model.eval()\n    with torch.no_grad():\n        out_data = model(batch_sample)\n        ps = torch.exp(out_data)\n        pred = ps.max(1).indices.cpu().numpy()\n        res = [le.inverse_transform([i])[0] for i in pred]\n    \n    if output == 'list':\n        return res\n    \n    data_tuple = zip(batch, res)\n    \n    if output == 'df':\n        return pd.DataFrame(data_tuple, columns=['name', 'prediction'])\n    \n    if output == 'dict':\n        return dict(data_tuple)\n\n\npredict(model, new_sample, output='dict', mask_inference=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/data-dl-final/test_DTDM.csv', sep=',', encoding='utf8', on_bad_lines='skip')\ntest_sample = df.name.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\n\nres = predict(model, test_sample, mask_inference=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = predict(model, test_sample, mask_inference=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res['prediction'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wrong_sample = res[res['prediction'] != 'Điện tử - Điện máy']['name'].to_list()\nwrong_sample\n\nif wrong_sample:\n    print(predict(model, wrong_sample, output = 'dict'))\nwrong_sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wrong_sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random = ['áo choàng đông']\npredict(model,random , output = 'dict')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.state_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'classifier.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('label_encoder.pkl', 'wb') as encoder:\n    pickle.dump(le, encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}