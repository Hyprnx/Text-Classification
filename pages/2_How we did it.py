import streamlit as st

st.header("This is how we did it")
st.markdown("# Data Normalization!")

st.markdown("""
    ## About the dataset
    The data we used were crawled from the ecomerce site [Shopee](https://shopee.vn/). The data were in the form of a
    csv file with 2 columns: product name and category. 
    """)


st.markdown("""
    ## The problem
    ### Different text font
    The online ecommerce platform consists of many seller, they be there, with a ultimate goal, is to sell as much
    product as they possibly can. So that they will try to make their product as much appealling as possible. 
    For example, like this:
    
    - ğ‘ºá»¯ğ’‚ ğ’“á»­ğ’‚ ğ’áº·ğ’• ğ‘ªğ’†ğ’“ğ’‚ğ’—ğ’† 236ğ’ğ’, 355ğ’ğ’, 473ğ’ğ’ ğ’„ğ’‰ğ’ ğ’…ğ’‚ ğ’…áº§ğ’–, ğ’…ğ’‚ ğ’Œğ’‰Ã´
    - ğš…Ã²ğš– ğ™²áº£ğš– á»¨ğš—ğš, ğš…Ã²ğš– ğ™¾ğš–ğšğšğšŠ ğ™³ğšğšŸğš˜ğš’, ğš‹á»• ğšœğšğš—ğš Ã¡ğš—ğš‘ ğšœÃ¡ğš—ğš ğ™²ğšŠğŸ¸+, ğšŒğš‘á»‘ğš—ğš ğš˜ğš¡ğš¢ ğš‘Ã³ğšŠ ğ™·ğš¢ğšğš›ğš˜ğšğšğš—
    
    These text are human-readable, we can translate the text to normal text just fine. But machine, they cant, every
    text will be translate to some type of unicode code, and these font above, is completely different from the normal
    text we use.
    
    ### Additional infomation and numbers
    More over, sellers even add additional infomation to the product name, like this:
    - [ğ‚ğ‡Ãğğ‡ ğ‡Ãƒğğ† ğŸ’¯%] ğğğƒğ˜ ğŒá»€ğŒ ğÆ¯á»šğ‚ ğ‡ğğ€ ğ€ğ‚ğğ’ğŒğ„ğ“ğˆğ‚ ğŸğŸğŸğŸ
    - ã€ï»¿[ã€€ï¼ï¼—ï¼ï¼’ï¼ï¼’ï¼“ã€€ï¼³ï¼¡ï¼¬ï¼¥ã€€ï¼³á»ï¼£ã€€]ã€€ï¼³á»¯ï½ã€€ï½’á»­ï½ã€€ï½áº·ï½”ã€€ï¼—ã€€ï½–á»‹ã€€ï¼¥ï¼£ï¼¯ï¼³ï¼¹ã€€ï¼¨Ã ï½ã€€ï¼±ï½•á»‘ï½ƒã€‘
    
    The different type of text, give human more infomation, but that is the infomation about the product, not their 
    categories, since whether the product is authentic or not, or it is on sale or not or how many products are in a 
    pack, does not matter with product category task, with both human and computer.""")


st.markdown("""
    ## Solution
    The solution is to build a preprocessor that can normalize the sample name to the type that we can use to train our
    model. The preprocessor will do the following:
    - Remove the additional infomation in bracket.
    - Normalize the unicode code using the [unicodedata](https://docs.python.org/3/library/unicodedata.html) library.
    
    The result show look like the following:
    
    
    |                |Original text                  |Text after preprocessed      |
    |----------------|-------------------------------|-----------------------------|
    |Product name	 |`ğ‘ºá»¯ğ’‚ ğ’“á»­ğ’‚ ğ’áº·ğ’• ğ‘ªğ’†ğ’“ğ’‚ğ’—ğ’† 236ğ’ğ’, 355ğ’ğ’, 473ğ’ğ’ ğ’„ğ’‰ğ’ ğ’…ğ’‚ ğ’…áº§ğ’–, ğ’…ğ’‚ ğ’Œğ’‰Ã´` |"Sá»¯a rá»­a máº·t Cerave cho da dáº§u khÃ´"|
    |	             |`ğš…Ã²ğš– ğ™²áº£ğš– á»¨ğš—ğš, ğš…Ã²ğš– ğ™¾ğš–ğšğšğšŠ ğ™³ğšğšŸğš˜ğš’, ğš‹á»• ğšœğšğš—ğš Ã¡ğš—ğš‘ ğšœÃ¡ğš—ğš ğ™²ğšŠğŸ¸+, ğšŒğš‘á»‘ğš—ğš ğš˜ğš¡ğš¢ ğš‘Ã³ğšŠ ğ™·ğš¢ğšğš›ğš˜ğšğšğš—`|"VÃ²m Cáº£m á»¨ng VÃ²m Omega Devoi bá»• sung Ã¡nh sÃ¡ng chá»‘ng oxy hÃ³a Hydrogen"|
    |				 |`-[ğ‚ğ‡Ãğğ‡ ğ‡Ãƒğğ† ğŸ’¯%] ğğğƒğ˜ ğŒá»€ğŒ ğÆ¯á»šğ‚ ğ‡ğğ€ ğ€ğ‚ğğ’ğŒğ„ğ“ğˆğ‚ ğŸğŸğŸğŸ- `|"Body má»m nÆ°á»›c hoa ACOSMETIC"|
    
    """)

st.markdown("# Calculate Text Embeddings!")
st.markdown("""
    ## What is text embedding?
    Computer can only understand numbers, so we need to convert text to some sort of numbers, and that is what text embedding is for.
    Text embedding is a way to represent text in a vector space, so that we can use the vector to train our model.
""")

st.markdown("""
        ## What text embedding method did we use?
        We did try a lot of text embedding methods, since calculating text embedding can be done in various methods. But the methods should be
        fast, reliable, scalable and perhap should be paralellizable. We did try the following methods: 
        - [Vietnamese Sentence Bi-directional Encoder Representations from Transformers ( Vietnamese sBERT)](https://huggingface.co/keepitreal/vietnamese-sbert) from HuggingFace.
        - [phoBERT](https://huggingface.co/vinai/phobert-base) from HuggingFace.
        - [TF-IDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from Scikit-learn.
        - [Count Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from Scikit-learn.
""")

st.markdown("# Build, train and validate models!")
st.markdown("""
    ## What model did we use?
    We expermented with a lot of models, but the models should be fast, reliable, and scalable. We did try the following models:
    - [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), 
        [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html),
        [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html),
        [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html),
        [Support Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from Scikit-learn.
    - Custom build Neural Network using [Tensorflow Keras](https://keras.io/api/models/sequential/) and [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).
""")

st.markdown("""
    ## The result
    All model tested on different type of text embedding method, and the result is not much different, archiving high
     8x% to 9x% accuracy. In our opinion, choosing what model is not really important, since the result is not 
     much different, but choosing the right model that ready for production environment is. 
""")
